{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81f938d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "import scispacy\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b6069",
   "metadata": {},
   "source": [
    "### XML Structure\n",
    "- teiHeader\n",
    "    - fileDesc\n",
    "        - titleStmt\n",
    "            - title\n",
    "        - publicationStmt\n",
    "            - date\n",
    "        - sourceDesc\n",
    "            - biblStruct\n",
    "                - idno\n",
    "    - encodingDesc\n",
    "    - profileDesc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e4ba03",
   "metadata": {},
   "source": [
    "### XRI (XML-to-RDF-Intermediate) Data Structure\n",
    "- [\n",
    "    - [\n",
    "        - section_number,\n",
    "        - section_title,\n",
    "        - {\n",
    "            - paragraph_number: paragraph_text,\n",
    "        - }\n",
    "    - ],\n",
    "- ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2509702",
   "metadata": {},
   "source": [
    "# RDF Generator\n",
    "### Reading XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47f3abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_reader():\n",
    "\n",
    "    # LOADING XML AND CREATING ROOT\n",
    "\n",
    "    xml_file = 'Robles_2015_modified.xml'\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # EXTRACTING METADATA (available data: title, publication_date and doi)\n",
    "\n",
    "    metadata = []\n",
    "\n",
    "    for elem in root:\n",
    "        if elem.tag[29:] == 'teiHeader':\n",
    "            for sub_elem1 in elem:\n",
    "                if sub_elem1.tag[29:] == 'fileDesc':\n",
    "                    publication_info = {}\n",
    "                    for sub_elem2 in sub_elem1:\n",
    "                        if sub_elem2.tag[29:] == 'titleStmt':\n",
    "                            for sub_elem3 in sub_elem2:\n",
    "                                if sub_elem3.tag[29:] == 'title':\n",
    "                                    publication_info['Title'] = sub_elem3.text\n",
    "                        if sub_elem2.tag[29:] == 'publicationStmt':\n",
    "                            for sub_elem3 in sub_elem2:\n",
    "                                if sub_elem3.tag[29:] == 'date':\n",
    "                                    publication_info['Publication Date'] = sub_elem3.attrib['when']\n",
    "                        if sub_elem2.tag[29:] == 'sourceDesc':\n",
    "                            for sub_elem3 in sub_elem2:\n",
    "                                if sub_elem3.tag[29:] == 'biblStruct':\n",
    "                                    for sub_elem4 in sub_elem3:\n",
    "                                        if sub_elem4.tag[29:] == 'idno':\n",
    "                                            publication_info['DOI'] = sub_elem4.text\n",
    "                    metadata.append(['0', 'Metadata', publication_info])\n",
    "\n",
    "#     print(metadata)\n",
    "    #==================================================================\n",
    "    \n",
    "    # EXTRACTING ABSTRACT AND NUMBERING PARAGRAPHS\n",
    "\n",
    "    abstract = []\n",
    "\n",
    "    for elem in root:\n",
    "        if elem.tag[29:] == 'teiHeader':\n",
    "            for sub_elem1 in elem:\n",
    "                if sub_elem1.tag[29:] == 'profileDesc':\n",
    "                    for sub_elem2 in sub_elem1:\n",
    "                        if sub_elem2.tag[29:] == 'abstract':\n",
    "                            for sub_elem3 in sub_elem2:\n",
    "                                if sub_elem3.tag[29:] == 'div':\n",
    "                                    list_of_paragraphs = {}\n",
    "                                    paragraph_number = 0\n",
    "                                    for sub_elem4 in sub_elem3:\n",
    "                                        if sub_elem4.tag[29:] == 'p':\n",
    "                                            paragraph_number += 1\n",
    "                                            list_of_paragraphs[paragraph_number] = ET.tostring(sub_elem4, encoding='unicode')\n",
    "                                    abstract.append(['0', 'Abstract', list_of_paragraphs])\n",
    "\n",
    "#     print(abstract)\n",
    "    #==================================================================\n",
    "\n",
    "    # EXTRACTING OTHER SECTIONS\n",
    "\n",
    "    # need to normalize the section number (line 17)\n",
    "    # compare Rojas and Wolf section number\n",
    "    # \\ issue in RDF, see Koshkava 2014 paper\n",
    "\n",
    "    list_of_sections = []\n",
    "\n",
    "    for elem in root:\n",
    "        if (elem.tag[29:] == 'text'):\n",
    "            for sub_elem1 in elem:\n",
    "                if (sub_elem1.tag[29:] == 'body'):\n",
    "                    for sub_elem2 in sub_elem1:\n",
    "                        if sub_elem2.tag[29:] == 'div':\n",
    "                            section_number = ''\n",
    "                            section_name = ''\n",
    "                            list_of_paragraphs = []\n",
    "                            for sub_elem3 in sub_elem2:\n",
    "                                if sub_elem3.tag[29:] == 'head':\n",
    "                                    if bool(sub_elem3.attrib):\n",
    "                                        section_number = str(sub_elem3.attrib)\n",
    "                                        if section_number[-3] == '.':\n",
    "                                            section_number = section_number[7:-3]\n",
    "                                        else:\n",
    "                                            section_number = section_number[7:-2]\n",
    "                                    else:\n",
    "                                        section_number = 'NO_SECTION_NUMBER'\n",
    "                                    section_name = sub_elem3.text\n",
    "                                if sub_elem3.tag[29:] == 'p':\n",
    "                                    list_of_paragraphs.append(ET.tostring(sub_elem3, encoding='unicode'))\n",
    "                            # commented logic skips NO_SECTION_NUMBER with no paragraphs, i.e., Table 1, Table 2, .....\n",
    "                            # if section_number == 'NO_SECTION_NUMBER' and not bool(list_of_paragraphs):\n",
    "                            if section_number == 'NO_SECTION_NUMBER':\n",
    "                                pass\n",
    "                            else:\n",
    "                                list_of_sections.append([section_number, section_name, list_of_paragraphs])\n",
    "\n",
    "#     print(list_of_sections)\n",
    "    #==================================================================\n",
    "\n",
    "    # NUMBERING PARAGRAPHS OF SECTIONS \n",
    "    # paragraphs are sequentially added in a dictionary\n",
    "    # dictionary is added with each respective record as a 4th element\n",
    "\n",
    "    # for section in list_of_sections:\n",
    "    #     list_of_paragraphs = {}\n",
    "    #     for paragraph_number, paragraph_text in enumerate(section[2], start=1):\n",
    "    #         list_of_paragraphs[paragraph_number] = paragraph_text\n",
    "    #     section.append(list_of_paragraphs)\n",
    "\n",
    "\n",
    "    # # deleting the 3rd element from each record\n",
    "\n",
    "    # for section in list_of_sections:\n",
    "    #     section.remove(section[2])\n",
    "\n",
    "\n",
    "    # OPTIMIZED VERSION\n",
    "    for section in list_of_sections:\n",
    "        dict_ = {i + 1: section[2][i] for i in range(len(section[2]))}\n",
    "        section.append(dict_)\n",
    "        section.remove(section[2])\n",
    "\n",
    "#     print(list_of_sections)\n",
    "    #==================================================================\n",
    "    \n",
    "    # MERGING ABSTRACT WITH OTHER SECTIONS\n",
    "\n",
    "    document = abstract + list_of_sections\n",
    "#     print(document)\n",
    "    #==================================================================\n",
    "\n",
    "    # PREPROCESSING TEXT\n",
    "\n",
    "    starting_p_tag_pattern = r'<ns0:p[^>]+>'\n",
    "    ending_p_tag_pattern = '</ns0:p>'\n",
    "    starting_ref_tag_pattern = r'<ns0:ref[^>]+>'\n",
    "    ending_ref_tag_pattern = '</ns0:ref>'\n",
    "    ref_pattern = r'<ref>.*?</ref>'          # temporary for removing ref tag\n",
    "\n",
    "    for record in document:\n",
    "        for paragraph_number, paragraph_text in record[2].items():\n",
    "            text = paragraph_text\n",
    "            text = re.sub(starting_p_tag_pattern, '', text)\n",
    "            text = re.sub(ending_p_tag_pattern, '', text)\n",
    "            text = re.sub(starting_ref_tag_pattern, '<ref>', text)\n",
    "            text = re.sub(ending_ref_tag_pattern, '</ref>', text)\n",
    "            text = re.sub(ref_pattern, '', text)\n",
    "            record[2][paragraph_number] = text\n",
    "\n",
    "#     print(document)\n",
    "    #==================================================================\n",
    "    \n",
    "    # MERGING METADATA WITH OTHER DOCUMENT PARTS\n",
    "\n",
    "    document = metadata + document\n",
    "#     print(document)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dfe8b2",
   "metadata": {},
   "source": [
    "### Necessary Functions for:\n",
    "***document_representer()*** \\\n",
    "***entity_representer()***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d5ab8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "def prefix():\n",
    "\n",
    "    PREFIX_ONNER = \"PREFIX onner: <http://purl.org/spatialai/onner/onner-full#>\\n\"\n",
    "    PREFIX_DATA = \"PREFIX data: <http://purl.org/spatialai/onner/onner-full/data#>\\n\"\n",
    "    PREFIX_RDF = \"PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\\n\"\n",
    "    PREFIX_RDFS = \"PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n\"\n",
    "    PREFIX_XSD = \"PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\\n\"\n",
    "    PREFIX_OWL = \"PREFIX owl: <http://www.w3.org/2002/07/owl#>\\n\\n\"\n",
    "    \n",
    "    return PREFIX_ONNER + PREFIX_DATA + PREFIX_RDF + PREFIX_RDFS + PREFIX_XSD + PREFIX_OWL\n",
    "\n",
    "\n",
    "def nested_section_check(document, doi, section_number):\n",
    "\n",
    "    section_ids = []\n",
    "\n",
    "    for record in document:\n",
    "        pattern = rf'^{section_number}\\.[^.]+$'\n",
    "        \n",
    "        if re.search(pattern, record[0]):\n",
    "            section_id = doi + '_' + str(record[0])\n",
    "            section_ids.append('data:'+section_id)\n",
    "\n",
    "    section_ids_joined = ', '.join(section_ids)\n",
    "\n",
    "    return section_ids_joined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61142a70",
   "metadata": {},
   "source": [
    "### RDF - Document Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "053b44bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def document_representer(document):\n",
    "    # WRITING RDF TRIPLES FOR DOCUMENT\n",
    "\n",
    "    # GETTING DOI\n",
    "    try:\n",
    "        doi = document[0][2]['DOI']\n",
    "        doi = doi.replace('/', '_')\n",
    "    except NameError:\n",
    "        print('ERROR: Document object is not defined!')\n",
    "\n",
    "    # PRINTING PREFIXES\n",
    "    rdf_triple = ''\n",
    "\n",
    "    # try:\n",
    "    for record in document:\n",
    "\n",
    "        # METADATA\n",
    "        if record[1] == 'Metadata':\n",
    "\n",
    "            abstract_id = 'data:' + doi + '_A'\n",
    "            section_ids = []\n",
    "\n",
    "            for section_number in document:\n",
    "                if section_number[0] != '0' and '.' not in section_number[0]:\n",
    "                    section_id = doi + '_' + section_number[0]\n",
    "                    section_ids.append('data:'+section_id)\n",
    "\n",
    "            directly_contained_sections = ', '.join(section_ids)\n",
    "\n",
    "            # PRINTING METADATA\n",
    "            rdf_triple += f\"data:Publication_{doi} rdf:type onner:ScholarlyPublication ;\\n\"\n",
    "            rdf_triple += f\"onner:publicationTitle '{record[2]['Title']}'^^xsd:string ;\\n\"\n",
    "            rdf_triple += f\"onner:publicationDate '{record[2]['Publication Date']}'^^xsd:date ;\\n\"\n",
    "            rdf_triple += f\"onner:doi '{record[2]['DOI']}'^^xsd:string ;\\n\"\n",
    "            rdf_triple += f\"onner:directlyContainsDocumentPart {abstract_id}, {directly_contained_sections} .\\n\\n\"\n",
    "\n",
    "        # ABSTRACT AND IT'S DOCUMENT PARTS\n",
    "        elif record[1] == 'Abstract':\n",
    "\n",
    "            next_index = document.index(record) + 1\n",
    "            next_section = document[next_index][0]\n",
    "            paragraph_ids = []\n",
    "\n",
    "            # CREATING PARAGRAPH ID FOR ABSTRACT'S PARAGRAPH \n",
    "            for paragraph_number, _ in record[2].items():\n",
    "                paragraph_id = doi + '_A' + '-' + str(paragraph_number)\n",
    "                paragraph_ids.append('data:'+paragraph_id)\n",
    "\n",
    "            paragraph_ids_joined = ', '.join(paragraph_ids)\n",
    "\n",
    "            # PRINTING ABSTRACT AND IT'S PARAGRAPH IDs \n",
    "            rdf_triple += f\"data:{doi}_A rdf:type onner:Abstract ;\\n\"\n",
    "            rdf_triple += f\"onner:nextDocumentPart {paragraph_ids[0]} ;\\n\"    # NEXT DOC PART AFTER ABSTRACT\n",
    "            rdf_triple += f\"onner:directlyContainsDocumentPart {paragraph_ids_joined} .\\n\\n\"\n",
    "\n",
    "            # PARAGRAPHS AND IT'S DOCUMENT PARTS\n",
    "            for paragraph_number, paragraph_text in record[2].items():\n",
    "\n",
    "                # replacing ' with \\' in text\n",
    "                if \"'\" in paragraph_text:\n",
    "                    paragraph_text = paragraph_text.replace(\"'\", r\"\\'\")\n",
    "\n",
    "                # PRINTING PARAGRAPHS AND IT'S LABELED TERM IDs \n",
    "                rdf_triple += f\"data:{doi}_A-{paragraph_number} rdf:type onner:Paragraph ;\\n\"\n",
    "                rdf_triple += f\"onner:positionInParentDocumentPart '{paragraph_number}'^^xsd:nonNegativeInteger ;\\n\"\n",
    "\n",
    "                if paragraph_number == len(paragraph_ids):\n",
    "                    rdf_triple += f\"onner:nextDocumentPart data:{doi}_{next_section} ;\\n\"\n",
    "                else:\n",
    "                    rdf_triple += f\"onner:nextDocumentPart data:{doi}_A-{paragraph_number+1} ;\\n\"\n",
    "\n",
    "                rdf_triple += f\"onner:paragraphText '{paragraph_text}'^^xsd:string .\\n\\n\"\n",
    "\n",
    "        # SECTION AND IT'S DOCUMENT PARTS\n",
    "        else:\n",
    "            section_number = record[0]\n",
    "            section_name = record[1]\n",
    "            next_index = document.index(record) + 1\n",
    "            paragraph_ids = []\n",
    "\n",
    "            if next_index == len(document):\n",
    "                next_section = 'EndOfDocument'\n",
    "            else:\n",
    "                next_section = document[next_index][0]\n",
    "\n",
    "            # IF N0 PARAGRAPHS EXIST BETWEEN A SECTION AND ITS IMMEDIATE SUBSECTION\n",
    "            if not bool(record[2]):\n",
    "\n",
    "                directly_contained_sections = nested_section_check(document, doi, section_number)\n",
    "\n",
    "                rdf_triple += f\"data:{doi}_{section_number} rdf:type onner:Section ;\\n\"\n",
    "                rdf_triple += f\"onner:sectionTitle '{section_name}'^^xsd:string ;\\n\"\n",
    "                rdf_triple += f\"onner:sectionNumber '{section_number}'^^xsd:string ;\\n\"\n",
    "                rdf_triple += f\"onner:nextDocumentPart data:{doi}_{next_section} ;\\n\"\n",
    "                rdf_triple += f\"onner:directlyContainsDocumentPart {directly_contained_sections} .\\n\\n\"\n",
    "\n",
    "            # IF PARAGRAPHS EXIST BETWEEN A SECTION AND ITS IMMEDIATE SUBSECTION\n",
    "            else:\n",
    "\n",
    "                directly_contained_sections = nested_section_check(document, doi, section_number)\n",
    "\n",
    "                # CREATING PARAGRAPH ID FOR SECTION'S PARAGRAPH \n",
    "                for paragraph_number, _ in record[2].items():\n",
    "                    paragraph_id = doi + '_' + str(section_number) + '-' + str(paragraph_number)\n",
    "                    paragraph_ids.append('data:'+paragraph_id)\n",
    "\n",
    "                paragraph_ids_joined = ', '.join(paragraph_ids)\n",
    "\n",
    "                # PRINTING SECTION AND IT'S PARAGRAPH IDs \n",
    "                rdf_triple += f\"data:{doi}_{section_number} rdf:type onner:Section ;\\n\"\n",
    "                rdf_triple += f\"onner:sectionTitle '{section_name}'^^xsd:string ;\\n\"\n",
    "                rdf_triple += f\"onner:sectionNumber '{section_number}'^^xsd:string ;\\n\"\n",
    "                rdf_triple += f\"onner:nextDocumentPart {paragraph_ids[0]} ;\\n\"    # NEXT DOC PART AFTER SECTION\n",
    "\n",
    "                if bool(directly_contained_sections):\n",
    "                    rdf_triple += f\"onner:directlyContainsDocumentPart {paragraph_ids_joined}, {directly_contained_sections} .\\n\\n\"\n",
    "                else:\n",
    "                    rdf_triple += f\"onner:directlyContainsDocumentPart {paragraph_ids_joined} .\\n\\n\"\n",
    "\n",
    "                # PARAGRAPHS AND IT'S DOCUMENT PARTS\n",
    "                for paragraph_number, paragraph_text in record[2].items():\n",
    "\n",
    "                    # replacing ' with \\' in text\n",
    "                    if \"'\" in paragraph_text:\n",
    "                        paragraph_text = paragraph_text.replace(\"'\", r\"\\'\")\n",
    "\n",
    "                    # PRINTING PARAGRAPHS AND IT'S LABELED TERM IDs \n",
    "                    rdf_triple += f\"data:{doi}_{section_number}-{paragraph_number} rdf:type onner:Paragraph ;\\n\"\n",
    "                    rdf_triple += f\"onner:positionInParentDocumentPart '{paragraph_number}'^^xsd:nonNegativeInteger ;\\n\"\n",
    "\n",
    "                    if paragraph_number == len(paragraph_ids):\n",
    "                        rdf_triple += f\"onner:nextDocumentPart data:{doi}_{next_section} ;\\n\"\n",
    "                    else:\n",
    "                        rdf_triple += f\"onner:nextDocumentPart data:{doi}_{section_number}-{paragraph_number+1} ;\\n\"\n",
    "\n",
    "                    rdf_triple += f\"onner:paragraphText '{paragraph_text}'^^xsd:string .\\n\\n\"\n",
    "\n",
    "    # except NameError:\n",
    "    #     print('ERROR: Document object is not defined!')\n",
    "\n",
    "    rdf_triple += f\"data:{doi}_EndOfDocument rdf:type onner:EndOfDocument .\\n\"\n",
    "\n",
    "    return doi, rdf_triple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f8cc5",
   "metadata": {},
   "source": [
    "### RDF - Named Entity Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0f920db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def entity_representer(entities):\n",
    "    rdf_triple = ''\n",
    "    labels_in_doc = []\n",
    "    labeling_schema = ['CHEMICAL', \n",
    "                       'MATERIAL', \n",
    "                       'STRUCTURE',\n",
    "                       'PROPERTY',\n",
    "                       'APPLICATION',\n",
    "                       'PROCESS',\n",
    "                       'EQUIPMENT',\n",
    "                       'MEASUREMENT',\n",
    "                       'ABBREVIATION']\n",
    "\n",
    "    for _ in entities:\n",
    "        paragraph_id = _[0]\n",
    "        labeled_term_info_list = _[1]\n",
    "        labeled_term_ids_joined = _[2]\n",
    "\n",
    "        if bool(labeled_term_ids_joined):    # <= added logic for no labeled terms\n",
    "            rdf_triple += f\"data:{paragraph_id} onner:directlyContainsLabeledTerm {labeled_term_ids_joined} .\\n\\n\"\n",
    "        else:\n",
    "            rdf_triple += f\"data:{paragraph_id} onner:directlyContainsLabeledTerm data:NoLabeledTerm .\\n\\n\"\n",
    "\n",
    "        # LABELED TERMS\n",
    "        for info in labeled_term_info_list:\n",
    "            # PRINTING LABELED TERMS\n",
    "            rdf_triple += f\"data:{info[0]} rdf:type onner:LabeledTerm ;\\n\"    # DEAL WITH ATOMIC / COMPOUND\n",
    "            rdf_triple += f\"onner:labeledTermText '{info[1]}'^^xsd:string ;\\n\"\n",
    "            rdf_triple += f\"onner:offset '{info[3]}'^^xsd:nonNegativeInteger ;\\n\"\n",
    "            rdf_triple += f\"onner:length '{info[4]}'^^xsd:nonNegativeInteger ;\\n\"\n",
    "            rdf_triple += f\"onner:labeledTermDirectlyContainedBy data:{paragraph_id} ;\\n\"\n",
    "            rdf_triple += f\"onner:hasLabeledTermStatus data:Candidate_{info[0]} .\\n\\n\"\n",
    "\n",
    "            # PRINTING LABELED TERMS STATUS\n",
    "            rdf_triple += f\"data:Candidate_{info[0]} rdf:type onner:CandidateStatus ;\\n\"\n",
    "            rdf_triple += f\"onner:statusAssignmentDate '{info[5]}'^^xsd:dateTime ;\\n\"\n",
    "            rdf_triple += f\"onner:statusAssignedBy data:Cellulosic_NER_Model ;\\n\"\n",
    "\n",
    "            try:\n",
    "                if not bool(labeling_schema):\n",
    "                    raise Exception('Labeling schema is empty!')\n",
    "\n",
    "                label_number_in_schema = labeling_schema.index(info[2]) + 1\n",
    "            except Exception as e:\n",
    "                print(f'ERROR: {e}')\n",
    "            else:\n",
    "                rdf_triple += f\"onner:hasLabeledTermLabel data:Label_{label_number_in_schema} .\\n\\n\"\n",
    "\n",
    "            # adding lebels and their position in the schema\n",
    "            if [label_number_in_schema, info[2]] not in labels_in_doc:\n",
    "                labels_in_doc.append([label_number_in_schema, info[2]])\n",
    "\n",
    "    try:\n",
    "        if not bool(labels_in_doc):\n",
    "            raise Exception('List of labels found in document is empty!\\n')     \n",
    "        else:\n",
    "            for label in labels_in_doc:\n",
    "                rdf_triple += f\"data:Label_{label[0]} rdf:type onner:Label ;\\n\"\n",
    "                rdf_triple += f\"onner:fromLabelingSchema data:Labeling_Schema ;\\n\"\n",
    "                rdf_triple += f\"onner:labelText '{label[1]}'^^xsd:string .\\n\\n\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'ERROR: {e}')\n",
    "\n",
    "    rdf_triple += f\"data:Labeling_Schema rdf:type onner:LabelingSchema ;\\n\"\n",
    "    rdf_triple += f\"onner:schemaName 'CelloGraph'^^xsd:string .\\n\\n\"\n",
    "\n",
    "    rdf_triple += f\"data:Cellulosic_NER_Model rdf:type onner:NER_System ;\\n\"    # if/else required to identify system and human\n",
    "    rdf_triple += f\"onner:systemVersion '1.0'^^xsd:string .\\n\\n\"\n",
    "\n",
    "    return rdf_triple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91f3e5",
   "metadata": {},
   "source": [
    "# GraphDB Im- and Exporter\n",
    "### Insert Graph into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69bf5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, POST\n",
    "\n",
    "def import_graph(document_id, rdf_triple, rdf_type):\n",
    "\n",
    "    repository_name = 'test-repo'\n",
    "    database_url = f'http://localhost:7200/repositories/{repository_name}/statements'\n",
    "    \n",
    "    if rdf_type == 'document':\n",
    "        named_graph = f'http://purl.org/spatialai/onner/data/{document_id}/document'\n",
    "    elif rdf_type == 'entities':\n",
    "        named_graph = f'http://purl.org/spatialai/onner/data/{document_id}/terms'\n",
    "\n",
    "    # Construct SPARQL INSERT query with explicit prefixes and a named graph\n",
    "    query = f'''\n",
    "        {prefix()}\n",
    "\n",
    "        INSERT DATA {{\n",
    "            GRAPH <{named_graph}> {{\n",
    "                {rdf_triple}\n",
    "            }}\n",
    "        }}\n",
    "    '''\n",
    "\n",
    "    # Execute via SPARQLWrapper\n",
    "    sparql = SPARQLWrapper(database_url)\n",
    "    sparql.setMethod(POST)\n",
    "    sparql.setQuery(query)\n",
    "\n",
    "    try:\n",
    "        sparql.query()\n",
    "        print(f'✅ RDF ({rdf_type}) successfully inserted into - {named_graph}')\n",
    "    except Exception as e:\n",
    "        print(f'❌ Error: {e}')\n",
    "        \n",
    "#     return named_graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc90a5",
   "metadata": {},
   "source": [
    "### Retrieve Paragraphs from Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "122e6472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def retrieve_paragraphs(document_id):\n",
    "    # connect and query graph database\n",
    "    # ADD TRY CATCH TO AVOID DATABASE CONNECTIVITY ERROR\n",
    "\n",
    "    # specify the repository\n",
    "    sparql = SPARQLWrapper(\"http://localhost:7200/repositories/test-repo\")\n",
    "\n",
    "    # SPARQL query\n",
    "    # SPARQL query with named graph and error handling\n",
    "    try:\n",
    "        sparql.setQuery(f'''\n",
    "            {prefix()}\n",
    "\n",
    "            SELECT ?paragraphId ?paragraphText \n",
    "            WHERE {{\n",
    "                    data:Publication_{document_id} rdf:type onner:ScholarlyPublication ;\n",
    "                                   onner:containsDocumentPart ?paragraphId .\n",
    "\n",
    "                    ?paragraphId rdf:type onner:Paragraph ;\n",
    "                                 onner:paragraphText ?paragraphText .\n",
    "            }}\n",
    "        ''')\n",
    "\n",
    "        # convert results to JSON\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        fetched_data = sparql.query().convert()\n",
    "    except Exception as e:\n",
    "        print(f'Error querying the SPARQL endpoint: {e}')\n",
    "\n",
    "    return fetched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb74563c",
   "metadata": {},
   "source": [
    "# NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e923775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(paragraph_text):\n",
    "    \n",
    "    try:\n",
    "        model_path = '/home/umayer/Work/_dev'\n",
    "        model_name = 'output_sci_md_vector/model-best'\n",
    "        nlp = spacy.load(f'{model_path}/{model_name}')\n",
    "    except OSError:\n",
    "        print('ERROR: Model Not Found!')\n",
    "    else:\n",
    "        print('Model loaded successfully.')\n",
    "\n",
    "    doc = nlp(paragraph_text)\n",
    "    \n",
    "    print('Object created successfully.')\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf0067",
   "metadata": {},
   "source": [
    "# NER Tool Integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ca935d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Extract paragraphs from Graph DB produced data object\n",
    "def get_paragraphs(fetched_data):\n",
    "    \n",
    "    paragraphs = []\n",
    "\n",
    "    for paragraph in fetched_data['results']['bindings']:\n",
    "        paragraph_id = paragraph['paragraphId']['value'].split('#')[1]\n",
    "        paragraph_text = paragraph['paragraphText']['value']\n",
    "        paragraphs.append([paragraph_id, paragraph_text, {'entities': []}])\n",
    "        \n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "# Create JSON with empty annotation for the annotation tool \n",
    "def create_empty_annotation():\n",
    "    \n",
    "    labels = [\n",
    "        {'id': 1, 'name': 'CHEMICAL', 'color': 'red-11'},\n",
    "        {'id': 2, 'name': 'MATERIAL', 'color': 'red-11'},\n",
    "        {'id': 3, 'name': 'STRUCTURE', 'color': 'red-11'},\n",
    "        {'id': 4, 'name': 'PROPERTY', 'color': 'red-11'},\n",
    "        {'id': 5, 'name': 'APPLICATION', 'color': 'red-11'},\n",
    "        {'id': 6, 'name': 'PROCESS', 'color': 'red-11'},\n",
    "        {'id': 7, 'name': 'EQUIPMENT', 'color': 'red-11'},\n",
    "        {'id': 8, 'name': 'MEASUREMENT', 'color': 'red-11'},\n",
    "        {'id': 9, 'name': 'ABBREVIATION', 'color': 'red-11'}\n",
    "    ]\n",
    "    \n",
    "    annotations = {'classes': labels, 'annotations': get_paragraphs()}\n",
    "    json_data = json.dumps(annotations)\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "\n",
    "# \n",
    "def entity_generator(paragraphs):\n",
    "    \n",
    "    entities = list()\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_id = paragraph[0]\n",
    "        paragraph_text = paragraph[1]\n",
    "    \n",
    "        print(f'Processing paragraph: {paragraph_id}')\n",
    "        doc = ner(paragraph_text)\n",
    "        \n",
    "        all_entities = []\n",
    "        entity_number = 1\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            date_time = str(datetime.now())[:-7]    # RECHECK THE APPROPRIATE PLACEMENT\n",
    "            entity_id = paragraph_id + '-' + str(entity_number)\n",
    "            entity = ent.text\n",
    "            label = ent.label_\n",
    "            offset = ent.start_char\n",
    "            length = ent.end_char - ent.start_char\n",
    "            entity_info = [entity_id, entity, label, offset, length, date_time]\n",
    "            all_entities.append(entity_info)\n",
    "            entity_number += 1\n",
    "\n",
    "        entity_ids = ['data:'+i[0] for i in all_entities]\n",
    "        entity_ids_str = ', '.join(entity_ids) \n",
    "        entities.append([paragraph_id, all_entities, entity_ids_str])\n",
    "        \n",
    "        print('Entities processed successfully.')\n",
    "        \n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3cabc8",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Main File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43a77873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RDF (document) successfully inserted into - http://purl.org/spatialai/onner/data/10.1016_j.indcrop.2015.03.075/document\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_A-1\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_1-1\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_1-2\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_1-3\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_1-4\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_1-5\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_2.1-1\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_2.2-1\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_2.3-1\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_2.4-1\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_2.4-2\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_2.5-1\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_2.6-1\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_2.6-2\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_2.6-3\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_2.6-4\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_2.6-5\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_2.6-6\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_2.6-7\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_2.6-8\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_2.6-9\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_3.1-1\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_3.2-1\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_3.2-2\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_3.2-3\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_3.2-4\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_3.2-5\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_3.2-6\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_3.3-1\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_3.3-2\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_3.3-3\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "Processing paragraph: 10.1016_j.indcrop.2015.03.075_4-1\n",
      "Model loaded successfully.\n",
      "Object created successfully.\n",
      "Entities processed successfully.\n",
      "✅ RDF (terms) successfully inserted into - http://purl.org/spatialai/onner/data/10.1016_j.indcrop.2015.03.075/terms\n",
      "Execution time: 333.7047 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "document = xml_reader()\n",
    "document_id, rdf_triple = document_representer(document)\n",
    "import_graph(document_id, rdf_triple, 'document')\n",
    "fetched_data = retrieve_paragraphs(document_id)\n",
    "para = get_paragraphs(fetched_data)\n",
    "ent = entity_generator(para)\n",
    "rdf_triple_terms = entity_representer(ent)\n",
    "import_graph(document_id, rdf_triple_terms, 'entities')\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "execution_time = end_time - start_time\n",
    "print(f'Execution time: {execution_time:.4f} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CelloGraph",
   "language": "python",
   "name": "cellograph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
