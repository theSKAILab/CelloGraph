{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f938d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "import scispacy\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b6069",
   "metadata": {},
   "source": [
    "### XML Structure\n",
    "- teiHeader\n",
    "    - fileDesc\n",
    "        - titleStmt\n",
    "            - title\n",
    "        - publicationStmt\n",
    "            - date\n",
    "        - sourceDesc\n",
    "            - biblStruct\n",
    "                - idno\n",
    "    - encodingDesc\n",
    "    - profileDesc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e4ba03",
   "metadata": {},
   "source": [
    "### XRI (XML-to-RDF-Intermediate) Data Structure\n",
    "- [\n",
    "    - [\n",
    "        - section_number,\n",
    "        - section_title,\n",
    "        - {\n",
    "            - paragraph_number: paragraph_text,\n",
    "        - }\n",
    "    - ],\n",
    "- ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2509702",
   "metadata": {},
   "source": [
    "# RDF Generator\n",
    "### Reading XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f3abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml(file_name):\n",
    "\n",
    "    # LOADING XML AND CREATING ROOT\n",
    "    xml_file = f'{file_name}'\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # EXTRACTING METADATA (TITLE, PUBLICATION_DATE and DOI)\n",
    "    metadata = []\n",
    "\n",
    "    for elem in root:\n",
    "        if elem.tag[29:] == 'teiHeader':\n",
    "            for sub_elem1 in elem:\n",
    "                if sub_elem1.tag[29:] == 'fileDesc':\n",
    "                    publication_info = {}\n",
    "                    for sub_elem2 in sub_elem1:\n",
    "                        if sub_elem2.tag[29:] == 'titleStmt':\n",
    "                            for sub_elem3 in sub_elem2:\n",
    "                                if sub_elem3.tag[29:] == 'title':\n",
    "                                    publication_info['Title'] = sub_elem3.text\n",
    "                        if sub_elem2.tag[29:] == 'publicationStmt':\n",
    "                            for sub_elem3 in sub_elem2:\n",
    "                                if sub_elem3.tag[29:] == 'date':\n",
    "                                    publication_info['Publication Date'] = sub_elem3.attrib['when']\n",
    "                        if sub_elem2.tag[29:] == 'sourceDesc':\n",
    "                            for sub_elem3 in sub_elem2:\n",
    "                                if sub_elem3.tag[29:] == 'biblStruct':\n",
    "                                    for sub_elem4 in sub_elem3:\n",
    "                                        if sub_elem4.tag[29:] == 'idno':\n",
    "                                            publication_info['DOI'] = sub_elem4.text\n",
    "                    metadata.append(['0', 'Metadata', publication_info])\n",
    "    \n",
    "    # EXTRACTING ABSTRACT AND NUMBERING THE PARAGRAPHS\n",
    "    abstract = []\n",
    "\n",
    "    for elem in root:\n",
    "        if elem.tag[29:] == 'teiHeader':\n",
    "            for sub_elem1 in elem:\n",
    "                if sub_elem1.tag[29:] == 'profileDesc':\n",
    "                    for sub_elem2 in sub_elem1:\n",
    "                        if sub_elem2.tag[29:] == 'abstract':\n",
    "                            for sub_elem3 in sub_elem2:\n",
    "                                if sub_elem3.tag[29:] == 'div':\n",
    "                                    list_of_paragraphs = {}\n",
    "                                    paragraph_number = 0\n",
    "                                    for sub_elem4 in sub_elem3:\n",
    "                                        if sub_elem4.tag[29:] == 'p':\n",
    "                                            paragraph_number += 1\n",
    "                                            list_of_paragraphs[paragraph_number] = ET.tostring(sub_elem4, encoding='unicode')\n",
    "                                    abstract.append(['0', 'Abstract', list_of_paragraphs])\n",
    "                                 \n",
    "    # EXTRACTING SECTIONS\n",
    "    # need to normalize the section number (line 17)\n",
    "    # compare Rojas and Wolf section number\n",
    "    # \\ issue in RDF, see Koshkava 2014 paper\n",
    "    list_of_sections = []\n",
    "\n",
    "    for elem in root:\n",
    "        if (elem.tag[29:] == 'text'):\n",
    "            for sub_elem1 in elem:\n",
    "                if (sub_elem1.tag[29:] == 'body'):\n",
    "                    for sub_elem2 in sub_elem1:\n",
    "                        if sub_elem2.tag[29:] == 'div':\n",
    "                            section_number = ''\n",
    "                            section_name = ''\n",
    "                            list_of_paragraphs = []\n",
    "                            for sub_elem3 in sub_elem2:\n",
    "                                if sub_elem3.tag[29:] == 'head':\n",
    "                                    if bool(sub_elem3.attrib):\n",
    "                                        section_number = str(sub_elem3.attrib)\n",
    "                                        if section_number[-3] == '.':\n",
    "                                            section_number = section_number[7:-3]\n",
    "                                        else:\n",
    "                                            section_number = section_number[7:-2]\n",
    "                                    else:\n",
    "                                        section_number = 'NO_SECTION_NUMBER'\n",
    "                                    section_name = sub_elem3.text\n",
    "                                if sub_elem3.tag[29:] == 'p':\n",
    "                                    list_of_paragraphs.append(ET.tostring(sub_elem3, encoding='unicode'))\n",
    "                            # commented logic skips NO_SECTION_NUMBER with no paragraphs, i.e., Table 1, Table 2, .....\n",
    "                            # if section_number == 'NO_SECTION_NUMBER' and not bool(list_of_paragraphs):\n",
    "                            if section_number == 'NO_SECTION_NUMBER':\n",
    "                                pass\n",
    "                            else:\n",
    "                                list_of_sections.append([section_number, section_name, list_of_paragraphs])\n",
    "                                \n",
    "    # NUMBERING THE PARAGRAPHS OF SECTIONS \n",
    "    for section in list_of_sections:\n",
    "        dict_ = {i + 1: section[2][i] for i in range(len(section[2]))}\n",
    "        section.append(dict_)\n",
    "        section.remove(section[2])\n",
    "        \n",
    "    # MERGING ABSTRACT WITH OTHER DOCUMENT PARTS\n",
    "    document = abstract + list_of_sections\n",
    "\n",
    "    # PREPROCESSING TEXT\n",
    "    starting_p_tag_pattern = r'<ns0:p[^>]+>'\n",
    "    ending_p_tag_pattern = '</ns0:p>'\n",
    "    starting_ref_tag_pattern = r'<ns0:ref[^>]+>'\n",
    "    ending_ref_tag_pattern = '</ns0:ref>'\n",
    "    ref_pattern = r'<ref>.*?</ref>'          # temporary for removing ref tag\n",
    "\n",
    "    for record in document:\n",
    "        for paragraph_number, paragraph_text in record[2].items():\n",
    "            text = paragraph_text\n",
    "            text = re.sub(starting_p_tag_pattern, '', text)\n",
    "            text = re.sub(ending_p_tag_pattern, '', text)\n",
    "            text = re.sub(starting_ref_tag_pattern, '<ref>', text)\n",
    "            text = re.sub(ending_ref_tag_pattern, '</ref>', text)\n",
    "            text = re.sub(ref_pattern, '', text)\n",
    "            record[2][paragraph_number] = text\n",
    "    \n",
    "    # MERGING METADATA WITH OTHER DOCUMENT PARTS\n",
    "    document = metadata + document\n",
    "    \n",
    "    return document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dfe8b2",
   "metadata": {},
   "source": [
    "### Necessary Functions for:\n",
    "***represent_document()*** \\\n",
    "***represent_entity()***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5ab8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING PREFIXES\n",
    "def define_prefix():\n",
    "\n",
    "    PREFIX_ONNER = \"PREFIX onner: <http://purl.org/spatialai/onner/onner-full#>\\n\"\n",
    "    PREFIX_DATA = \"PREFIX data: <http://purl.org/spatialai/onner/onner-full/data#>\\n\"\n",
    "    PREFIX_RDF = \"PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\\n\"\n",
    "    PREFIX_RDFS = \"PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n\"\n",
    "    PREFIX_XSD = \"PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\\n\"\n",
    "    PREFIX_OWL = \"PREFIX owl: <http://www.w3.org/2002/07/owl#>\\n\\n\"\n",
    "    \n",
    "    return PREFIX_ONNER + PREFIX_DATA + PREFIX_RDF + PREFIX_RDFS + PREFIX_XSD + PREFIX_OWL\n",
    "\n",
    "\n",
    "# SUBSECTIONS CHECKER FOR A SECTION\n",
    "def check_nested_section(document, doi, section_number):\n",
    "\n",
    "    section_ids = []\n",
    "\n",
    "    for record in document:\n",
    "        pattern = rf'^{section_number}\\.[^.]+$'\n",
    "        \n",
    "        if re.search(pattern, record[0]):\n",
    "            section_id = doi + '_' + str(record[0])\n",
    "            section_ids.append('data:'+section_id)\n",
    "\n",
    "    section_ids_joined = ', '.join(section_ids)\n",
    "\n",
    "    return section_ids_joined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61142a70",
   "metadata": {},
   "source": [
    "### RDF - Document Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053b44bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RDF WRITER FOR DOCUMENT\n",
    "def represent_document(document):\n",
    "\n",
    "    # GETTING DOI\n",
    "    try:\n",
    "        doi = document[0][2]['DOI']\n",
    "        document_id = doi.replace('/', '_')\n",
    "    except NameError:\n",
    "        print('ERROR: Document object is not defined!')\n",
    "\n",
    "    # PRINTING PREFIXES\n",
    "    rdf_triple = ''\n",
    "\n",
    "    # WRITING RDF\n",
    "    for record in document:\n",
    "        # metadata\n",
    "        if record[1] == 'Metadata':\n",
    "            abstract_id = 'data:' + document_id + '_A'\n",
    "            section_ids = []\n",
    "\n",
    "            for section_number in document:\n",
    "                if section_number[0] != '0' and '.' not in section_number[0]:\n",
    "                    section_id = document_id + '_' + section_number[0]\n",
    "                    section_ids.append('data:'+section_id)\n",
    "\n",
    "            directly_contained_sections = ', '.join(section_ids)\n",
    "\n",
    "            rdf_triple += f\"data:Publication_{document_id} rdf:type onner:ScholarlyPublication ;\\n\"\n",
    "            rdf_triple += f\"onner:publicationTitle '{record[2]['Title']}'^^xsd:string ;\\n\"\n",
    "            rdf_triple += f\"onner:publicationDate '{record[2]['Publication Date']}'^^xsd:date ;\\n\"\n",
    "            rdf_triple += f\"onner:doi '{doi}'^^xsd:string ;\\n\"\n",
    "            rdf_triple += f\"onner:directlyContainsDocumentPart {abstract_id}, {directly_contained_sections} .\\n\\n\"\n",
    "\n",
    "        # abstract and it's document parts\n",
    "        elif record[1] == 'Abstract':\n",
    "            next_index = document.index(record) + 1\n",
    "            next_section = document[next_index][0]\n",
    "            paragraph_ids = []\n",
    "\n",
    "            for paragraph_number, _ in record[2].items():\n",
    "                paragraph_id = document_id + '_A' + '-' + str(paragraph_number)\n",
    "                paragraph_ids.append('data:'+paragraph_id)\n",
    "\n",
    "            paragraph_ids_joined = ', '.join(paragraph_ids)\n",
    "\n",
    "            rdf_triple += f\"data:{document_id}_A rdf:type onner:Abstract ;\\n\"\n",
    "            rdf_triple += f\"onner:nextDocumentPart {paragraph_ids[0]} ;\\n\"    # NEXT DOC PART AFTER ABSTRACT\n",
    "            rdf_triple += f\"onner:directlyContainsDocumentPart {paragraph_ids_joined} .\\n\\n\"\n",
    "\n",
    "            for paragraph_number, paragraph_text in record[2].items():\n",
    "                # replacing ' with \\' in text\n",
    "                if \"'\" in paragraph_text:\n",
    "                    paragraph_text = paragraph_text.replace(\"'\", r\"\\'\")\n",
    "\n",
    "                rdf_triple += f\"data:{document_id}_A-{paragraph_number} rdf:type onner:Paragraph ;\\n\"\n",
    "                rdf_triple += f\"onner:positionInParentDocumentPart '{paragraph_number}'^^xsd:nonNegativeInteger ;\\n\"\n",
    "\n",
    "                if paragraph_number == len(paragraph_ids):\n",
    "                    rdf_triple += f\"onner:nextDocumentPart data:{document_id}_{next_section} ;\\n\"\n",
    "                else:\n",
    "                    rdf_triple += f\"onner:nextDocumentPart data:{document_id}_A-{paragraph_number+1} ;\\n\"\n",
    "\n",
    "                rdf_triple += f\"onner:paragraphText '{paragraph_text}'^^xsd:string .\\n\\n\"\n",
    "\n",
    "        # section and it's document parts\n",
    "        else:\n",
    "            section_number = record[0]\n",
    "            section_name = record[1]\n",
    "            next_index = document.index(record) + 1\n",
    "            paragraph_ids = []\n",
    "\n",
    "            if next_index == len(document):\n",
    "                next_section = 'EndOfDocument'\n",
    "            else:\n",
    "                next_section = document[next_index][0]\n",
    "\n",
    "            # if no paragraphs exist between a section and its immediate subsection\n",
    "            # else paragraphs exist between a section and its immediate subsection\n",
    "            if not bool(record[2]):\n",
    "                directly_contained_sections = check_nested_section(document, document_id, section_number)\n",
    "\n",
    "                rdf_triple += f\"data:{document_id}_{section_number} rdf:type onner:Section ;\\n\"\n",
    "                rdf_triple += f\"onner:sectionTitle '{section_name}'^^xsd:string ;\\n\"\n",
    "                rdf_triple += f\"onner:sectionNumber '{section_number}'^^xsd:string ;\\n\"\n",
    "                rdf_triple += f\"onner:nextDocumentPart data:{document_id}_{next_section} ;\\n\"\n",
    "                rdf_triple += f\"onner:directlyContainsDocumentPart {directly_contained_sections} .\\n\\n\"\n",
    "            else:\n",
    "                directly_contained_sections = check_nested_section(document, document_id, section_number)\n",
    "\n",
    "                for paragraph_number, _ in record[2].items():\n",
    "                    paragraph_id = document_id + '_' + str(section_number) + '-' + str(paragraph_number)\n",
    "                    paragraph_ids.append('data:'+paragraph_id)\n",
    "\n",
    "                paragraph_ids_joined = ', '.join(paragraph_ids)\n",
    "\n",
    "                rdf_triple += f\"data:{document_id}_{section_number} rdf:type onner:Section ;\\n\"\n",
    "                rdf_triple += f\"onner:sectionTitle '{section_name}'^^xsd:string ;\\n\"\n",
    "                rdf_triple += f\"onner:sectionNumber '{section_number}'^^xsd:string ;\\n\"\n",
    "                rdf_triple += f\"onner:nextDocumentPart {paragraph_ids[0]} ;\\n\"    # NEXT DOC PART AFTER SECTION\n",
    "\n",
    "                if bool(directly_contained_sections):\n",
    "                    rdf_triple += f\"onner:directlyContainsDocumentPart {paragraph_ids_joined}, {directly_contained_sections} .\\n\\n\"\n",
    "                else:\n",
    "                    rdf_triple += f\"onner:directlyContainsDocumentPart {paragraph_ids_joined} .\\n\\n\"\n",
    "\n",
    "                for paragraph_number, paragraph_text in record[2].items():\n",
    "                    # replacing ' with \\' in text\n",
    "                    if \"'\" in paragraph_text:\n",
    "                        paragraph_text = paragraph_text.replace(\"'\", r\"\\'\")\n",
    " \n",
    "                    rdf_triple += f\"data:{document_id}_{section_number}-{paragraph_number} rdf:type onner:Paragraph ;\\n\"\n",
    "                    rdf_triple += f\"onner:positionInParentDocumentPart '{paragraph_number}'^^xsd:nonNegativeInteger ;\\n\"\n",
    "\n",
    "                    if paragraph_number == len(paragraph_ids):\n",
    "                        rdf_triple += f\"onner:nextDocumentPart data:{document_id}_{next_section} ;\\n\"\n",
    "                    else:\n",
    "                        rdf_triple += f\"onner:nextDocumentPart data:{document_id}_{section_number}-{paragraph_number+1} ;\\n\"\n",
    "\n",
    "                    rdf_triple += f\"onner:paragraphText '{paragraph_text}'^^xsd:string .\\n\\n\"\n",
    "\n",
    "    rdf_triple += f\"data:{document_id}_EndOfDocument rdf:type onner:EndOfDocument .\\n\"\n",
    "\n",
    "    return document_id, rdf_triple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f8cc5",
   "metadata": {},
   "source": [
    "### RDF - Named Entity Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f920db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RDF WRITER FOR NAMED ENTITIES\n",
    "def represent_entity(entities):\n",
    "    \n",
    "    rdf_triple = ''\n",
    "    labels_in_doc = []\n",
    "    labeling_schema = ['CHEMICAL', \n",
    "                       'MATERIAL', \n",
    "                       'STRUCTURE',\n",
    "                       'PROPERTY',\n",
    "                       'APPLICATION',\n",
    "                       'PROCESS',\n",
    "                       'EQUIPMENT',\n",
    "                       'MEASUREMENT',\n",
    "                       'ABBREVIATION']\n",
    "\n",
    "    # WRITING RDF\n",
    "    for i in entities:\n",
    "        paragraph_id = i[0]\n",
    "        labeled_term_info_list = i[1]\n",
    "#         labeled_term_ids_joined = i[2]\n",
    "\n",
    "        if bool(labeled_term_info_list):    # if no term found in paragraph\n",
    "            entity_ids = ['data:'+i[0] for i in labeled_term_info_list]\n",
    "            labeled_term_ids_joined = ', '.join(entity_ids) \n",
    "            \n",
    "            rdf_triple += f\"data:{paragraph_id} onner:directlyContainsLabeledTerm {labeled_term_ids_joined} .\\n\\n\"\n",
    "            \n",
    "            for info in labeled_term_info_list:\n",
    "                rdf_triple += f\"data:{info[0]} rdf:type onner:LabeledTerm ;\\n\"    # deal with atomic and compound terms\n",
    "                rdf_triple += f\"onner:labeledTermText '{info[1]}'^^xsd:string ;\\n\"\n",
    "                rdf_triple += f\"onner:offset '{info[3]}'^^xsd:nonNegativeInteger ;\\n\"\n",
    "                rdf_triple += f\"onner:length '{info[4]}'^^xsd:nonNegativeInteger ;\\n\"\n",
    "                rdf_triple += f\"onner:labeledTermDirectlyContainedBy data:{paragraph_id} ;\\n\"\n",
    "                rdf_triple += f\"onner:hasLabeledTermStatus data:Candidate_{info[0]} .\\n\\n\"\n",
    "\n",
    "                rdf_triple += f\"data:Candidate_{info[0]} rdf:type onner:CandidateStatus ;\\n\"\n",
    "                rdf_triple += f\"onner:statusAssignmentDate '{info[5]}'^^xsd:dateTime ;\\n\"\n",
    "                rdf_triple += f\"onner:statusAssignedBy data:Cellulosic_NER_Model ;\\n\"\n",
    "\n",
    "                try:\n",
    "                    if not bool(labeling_schema):\n",
    "                        raise Exception('Labeling schema is empty!')\n",
    "                    label_number_in_schema = labeling_schema.index(info[2]) + 1\n",
    "                except Exception as e:\n",
    "                    print(f'ERROR: {e}')\n",
    "                else:\n",
    "                    rdf_triple += f\"onner:hasLabeledTermLabel data:Label_{label_number_in_schema} .\\n\\n\"\n",
    "\n",
    "                # adding lebels and their positions in the schema\n",
    "                if [label_number_in_schema, info[2]] not in labels_in_doc:\n",
    "                    labels_in_doc.append([label_number_in_schema, info[2]])\n",
    "        else:\n",
    "            rdf_triple += f\"data:{paragraph_id} onner:directlyContainsLabeledTerm data:NoLabeledTerm .\\n\\n\"\n",
    "\n",
    "    try:\n",
    "        if not bool(labels_in_doc):\n",
    "            raise Exception('List of labels found in document is empty!\\n')     \n",
    "        else:\n",
    "            for label in labels_in_doc:\n",
    "                rdf_triple += f\"data:Label_{label[0]} rdf:type onner:Label ;\\n\"\n",
    "                rdf_triple += f\"onner:fromLabelingSchema data:Labeling_Schema ;\\n\"\n",
    "                rdf_triple += f\"onner:labelText '{label[1]}'^^xsd:string .\\n\\n\"\n",
    "    except Exception as e:\n",
    "        print(f'ERROR: {e}')\n",
    "\n",
    "    rdf_triple += f\"data:Labeling_Schema rdf:type onner:LabelingSchema ;\\n\"\n",
    "    rdf_triple += f\"onner:schemaName 'CelloGraph'^^xsd:string .\\n\\n\"\n",
    "\n",
    "    rdf_triple += f\"data:Cellulosic_NER_Model rdf:type onner:NER_System ;\\n\"    # if/else required to identify system and human\n",
    "    rdf_triple += f\"onner:systemVersion '1.0'^^xsd:string .\\n\\n\"\n",
    "\n",
    "    return rdf_triple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b8e70",
   "metadata": {},
   "source": [
    "### BACKUP CODE >> RDF - Named Entity Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fe43b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RDF WRITER FOR NAMED ENTITIES\n",
    "# def represent_entity(entities):\n",
    "    \n",
    "#     rdf_triple = ''\n",
    "#     labels_in_doc = []\n",
    "#     labeling_schema = ['CHEMICAL', \n",
    "#                        'MATERIAL', \n",
    "#                        'STRUCTURE',\n",
    "#                        'PROPERTY',\n",
    "#                        'APPLICATION',\n",
    "#                        'PROCESS',\n",
    "#                        'EQUIPMENT',\n",
    "#                        'MEASUREMENT',\n",
    "#                        'ABBREVIATION']\n",
    "\n",
    "#     # WRITING RDF\n",
    "#     for i in entities:\n",
    "#         paragraph_id = i[0]\n",
    "#         labeled_term_info_list = i[1]\n",
    "#         labeled_term_ids_joined = i[2]\n",
    "\n",
    "#         if bool(labeled_term_ids_joined):    # if no term found in paragraph\n",
    "#             rdf_triple += f\"data:{paragraph_id} onner:directlyContainsLabeledTerm {labeled_term_ids_joined} .\\n\\n\"\n",
    "#         else:\n",
    "#             rdf_triple += f\"data:{paragraph_id} onner:directlyContainsLabeledTerm data:NoLabeledTerm .\\n\\n\"\n",
    "\n",
    "#         for info in labeled_term_info_list:\n",
    "#             rdf_triple += f\"data:{info[0]} rdf:type onner:LabeledTerm ;\\n\"    # deal with atomic and compound terms\n",
    "#             rdf_triple += f\"onner:labeledTermText '{info[1]}'^^xsd:string ;\\n\"\n",
    "#             rdf_triple += f\"onner:offset '{info[3]}'^^xsd:nonNegativeInteger ;\\n\"\n",
    "#             rdf_triple += f\"onner:length '{info[4]}'^^xsd:nonNegativeInteger ;\\n\"\n",
    "#             rdf_triple += f\"onner:labeledTermDirectlyContainedBy data:{paragraph_id} ;\\n\"\n",
    "#             rdf_triple += f\"onner:hasLabeledTermStatus data:Candidate_{info[0]} .\\n\\n\"\n",
    "\n",
    "#             rdf_triple += f\"data:Candidate_{info[0]} rdf:type onner:CandidateStatus ;\\n\"\n",
    "#             rdf_triple += f\"onner:statusAssignmentDate '{info[5]}'^^xsd:dateTime ;\\n\"\n",
    "#             rdf_triple += f\"onner:statusAssignedBy data:Cellulosic_NER_Model ;\\n\"\n",
    "\n",
    "#             try:\n",
    "#                 if not bool(labeling_schema):\n",
    "#                     raise Exception('Labeling schema is empty!')\n",
    "#                 label_number_in_schema = labeling_schema.index(info[2]) + 1\n",
    "#             except Exception as e:\n",
    "#                 print(f'ERROR: {e}')\n",
    "#             else:\n",
    "#                 rdf_triple += f\"onner:hasLabeledTermLabel data:Label_{label_number_in_schema} .\\n\\n\"\n",
    "\n",
    "#             # adding lebels and their positions in the schema\n",
    "#             if [label_number_in_schema, info[2]] not in labels_in_doc:\n",
    "#                 labels_in_doc.append([label_number_in_schema, info[2]])\n",
    "\n",
    "#     try:\n",
    "#         if not bool(labels_in_doc):\n",
    "#             raise Exception('List of labels found in document is empty!\\n')     \n",
    "#         else:\n",
    "#             for label in labels_in_doc:\n",
    "#                 rdf_triple += f\"data:Label_{label[0]} rdf:type onner:Label ;\\n\"\n",
    "#                 rdf_triple += f\"onner:fromLabelingSchema data:Labeling_Schema ;\\n\"\n",
    "#                 rdf_triple += f\"onner:labelText '{label[1]}'^^xsd:string .\\n\\n\"\n",
    "#     except Exception as e:\n",
    "#         print(f'ERROR: {e}')\n",
    "\n",
    "#     rdf_triple += f\"data:Labeling_Schema rdf:type onner:LabelingSchema ;\\n\"\n",
    "#     rdf_triple += f\"onner:schemaName 'CelloGraph'^^xsd:string .\\n\\n\"\n",
    "\n",
    "#     rdf_triple += f\"data:Cellulosic_NER_Model rdf:type onner:NER_System ;\\n\"    # if/else required to identify system and human\n",
    "#     rdf_triple += f\"onner:systemVersion '1.0'^^xsd:string .\\n\\n\"\n",
    "\n",
    "#     return rdf_triple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91f3e5",
   "metadata": {},
   "source": [
    "# GraphDB Im- and Exporter\n",
    "### Insert Graph into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69bf5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, POST\n",
    "\n",
    "# DATA INSERTION INTO GRAPH DB\n",
    "def insert_graph(document_id, rdf_triple, rdf_type):\n",
    "\n",
    "    repository_name = 'test-repo'\n",
    "    database_url = f'http://localhost:7200/repositories/{repository_name}/statements'\n",
    "    \n",
    "    if rdf_type == 'document':\n",
    "        named_graph = f'http://purl.org/spatialai/onner/data/{document_id}/document'\n",
    "    elif rdf_type == 'entities':\n",
    "        named_graph = f'http://purl.org/spatialai/onner/data/{document_id}/terms'\n",
    "\n",
    "    # query => insert data using named graph\n",
    "    query = f'''\n",
    "        {define_prefix()}\n",
    "\n",
    "        INSERT DATA {{\n",
    "            GRAPH <{named_graph}> {{\n",
    "                {rdf_triple}\n",
    "            }}\n",
    "        }}\n",
    "    '''\n",
    "\n",
    "    # execute query\n",
    "    sparql = SPARQLWrapper(database_url)\n",
    "    sparql.setMethod(POST)\n",
    "    sparql.setQuery(query)\n",
    "\n",
    "    try:\n",
    "        sparql.query()\n",
    "        print(f'✔️ RDF ({rdf_type}) successfully inserted into - {named_graph}')\n",
    "    except Exception as e:\n",
    "        print(f'❌ Error: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc90a5",
   "metadata": {},
   "source": [
    "### Retrieve Paragraphs from Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "122e6472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# DATA RETRIEVAL FROM GRAPH DB\n",
    "def get_document_content(document_id):\n",
    "    \n",
    "    # specify the repository\n",
    "    sparql = SPARQLWrapper(\"http://localhost:7200/repositories/test-repo\")\n",
    "\n",
    "    # query => retrieving data\n",
    "    try:\n",
    "        sparql.setQuery(f'''\n",
    "            {define_prefix()}\n",
    "\n",
    "            SELECT ?paragraphId ?paragraphText \n",
    "            WHERE {{\n",
    "                    data:Publication_{document_id} rdf:type onner:ScholarlyPublication ;\n",
    "                                   onner:containsDocumentPart ?paragraphId .\n",
    "\n",
    "                    ?paragraphId rdf:type onner:Paragraph ;\n",
    "                                 onner:paragraphText ?paragraphText .\n",
    "            }}\n",
    "        ''')\n",
    "        # convert results to JSON\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        exported_data = sparql.query().convert()\n",
    "    except Exception as e:\n",
    "        print(f'Error querying the SPARQL endpoint: {e}')\n",
    "\n",
    "    return exported_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb74563c",
   "metadata": {},
   "source": [
    "# NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e923775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# NLP MODEL LOADER\n",
    "def ner(paragraph_text):\n",
    "    \n",
    "    if not hasattr(ner, 'nlp'):\n",
    "        try:\n",
    "            model_name = '/home/umayer/Work/_dev/output_sci_md_vector/model-best'\n",
    "            ner.nlp = spacy.load(f'{model_name}')\n",
    "            print('✔️ Model loaded successfully.')\n",
    "        except OSError:\n",
    "            print('ERROR: Model Not Found!')\n",
    "\n",
    "    doc = ner.nlp(paragraph_text) \n",
    "    \n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf0067",
   "metadata": {},
   "source": [
    "# NER Tool Integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ca935d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# PARAGRAPHS EXTRACTOR FROM GRAPH DB DATA\n",
    "def get_paragraph(exported_data):\n",
    "    \n",
    "    paragraphs = []\n",
    "\n",
    "    for paragraph in exported_data['results']['bindings']:\n",
    "        paragraph_id = paragraph['paragraphId']['value'].split('#')[1]\n",
    "        paragraph_text = paragraph['paragraphText']['value']\n",
    "        paragraphs.append([paragraph_id, paragraph_text, {'entities': []}])\n",
    "        \n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "# JSON PRODUCER FOR ANNOTATION TOOL \n",
    "def create_empty_annotation():\n",
    "    \n",
    "    labels = [\n",
    "        {'id': 1, 'name': 'CHEMICAL', 'color': 'red-11'},\n",
    "        {'id': 2, 'name': 'MATERIAL', 'color': 'red-11'},\n",
    "        {'id': 3, 'name': 'STRUCTURE', 'color': 'red-11'},\n",
    "        {'id': 4, 'name': 'PROPERTY', 'color': 'red-11'},\n",
    "        {'id': 5, 'name': 'APPLICATION', 'color': 'red-11'},\n",
    "        {'id': 6, 'name': 'PROCESS', 'color': 'red-11'},\n",
    "        {'id': 7, 'name': 'EQUIPMENT', 'color': 'red-11'},\n",
    "        {'id': 8, 'name': 'MEASUREMENT', 'color': 'red-11'},\n",
    "        {'id': 9, 'name': 'ABBREVIATION', 'color': 'red-11'}\n",
    "    ]\n",
    "    \n",
    "    annotations = {'classes': labels, 'annotations': get_paragraph()}\n",
    "    json_data = json.dumps(annotations)\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "\n",
    "# NAMED ENTITIES GENERATOR\n",
    "def generate_entity(paragraphs):\n",
    "    \n",
    "    entities_in_document = list()\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_id = paragraph[0]\n",
    "        paragraph_text = paragraph[1]\n",
    "        \n",
    "        doc = ner(paragraph_text)\n",
    "        \n",
    "        entities_in_paragraph = []\n",
    "        entity_number = 1\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            date_time = str(datetime.now())[:-7]    # RECHECK THE APPROPRIATE PLACEMENT\n",
    "            entity_id = paragraph_id + '-' + str(entity_number)\n",
    "            entity = ent.text\n",
    "            label = ent.label_\n",
    "            offset = ent.start_char\n",
    "            length = ent.end_char - ent.start_char\n",
    "            entity_info = [entity_id, entity, label, offset, length, date_time]\n",
    "            entities_in_paragraph.append(entity_info)\n",
    "            entity_number += 1\n",
    "\n",
    "#         entity_ids = ['data:'+i[0] for i in entities_in_paragraph]\n",
    "#         entity_ids_str = ', '.join(entity_ids) \n",
    "#         entities_in_document.append([paragraph_id, entities_in_paragraph, entity_ids_str])\n",
    "        entities_in_document.append([paragraph_id, entities_in_paragraph])\n",
    "        \n",
    "        print(f'Entity processed for {paragraph_id}')\n",
    "        \n",
    "    return entities_in_document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3cabc8",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Main File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a77873",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ RDF (document) successfully inserted into - http://purl.org/spatialai/onner/data/10.1016_j.indcrop.2015.03.075/document\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umayer/Work/_venv/CelloGraph/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/umayer/Work/_venv/CelloGraph/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/umayer/Work/_venv/CelloGraph/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "document = read_xml('/home/umayer/_div/OnNER/evaluation/example/input/Publication_10.1016_j.indcrop.2015.03.075.xml')\n",
    "document_id, rdf_document = represent_document(document)\n",
    "insert_graph(document_id, rdf_document, 'document')\n",
    "exported_data = get_document_content(document_id)\n",
    "paragraphs = get_paragraph(exported_data)\n",
    "entities = generate_entity(paragraphs)\n",
    "rdf_entities = represent_entity(entities)\n",
    "insert_graph(document_id, rdf_entities, 'entities')\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "execution_time = end_time - start_time\n",
    "print(f'Execution time: {execution_time:.4f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640b46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8fe48c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CelloGraph",
   "language": "python",
   "name": "cellograph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
